import streamlit as st
import pandas as pd
import re
from io import BytesIO
from datetime import datetime
import zipfile

# ============== PAGE CONFIG ==============
st.set_page_config(page_title="DVS Tools", page_icon="üß≠", layout="wide")

# ============== HELPERS (shared) ==============
def parse_dna_file(file) -> pd.DataFrame:
    """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå .dna ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á‡∏Ñ‡∏π‡πà (berth_id, td_id) ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    - ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ //
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (inline comment) ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á // ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    """
    content = file.read()
    try:
        text = content.decode("utf-8", errors="ignore")
    except Exception:
        text = content.decode(errors="ignore")
    lines = text.splitlines()

    in_data = False
    rows = []
    for raw in lines:
        s = raw.strip()

        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ DATA
        if s.startswith("** DATA BEGINS HERE **"):
            in_data = True
            continue
        if not in_data or not s:
            continue

        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á/ metadata
        if s.startswith("Version") or s.startswith("berth_id"):
            continue

        # --- ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î ---
        if s.startswith("//"):
            continue

        # --- ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (inline comment) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ---
        cut = raw
        pos = cut.find("//")
        if pos > 0:  # ‡∏°‡∏µ // ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
            cut = cut[:pos]

        # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏ö/‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏ô)
        toks = [t.strip() for t in cut.split("\t") if t.strip() != ""]
        if not toks:
            continue

        berth_id = toks[0]
        td_id    = toks[-1]

        # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡πÄ‡∏®‡∏©‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡πÇ‡∏ú‡∏•‡πà‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å
        if str(berth_id).startswith("//"):
            continue

        rows.append((str(berth_id).strip(), str(td_id).strip()))

    df = pd.DataFrame(rows, columns=["berth_id", "td_id"])
    return df


def parse_manual_list(text: str) -> set:
    """‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ space/comma/newline ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ã‡πá‡∏ï‡∏Ç‡∏≠‡∏á‡∏£‡∏´‡∏±‡∏™"""
    tokens = re.split(r"[,\s]+", (text or "").strip())
    return {t for t in tokens if t}


def compare_sets(name: str, dna_set: set, ref_set: set) -> pd.DataFrame:
    """‡∏Ñ‡∏∑‡∏ô DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ status = MISSING_IN_DNA / EXTRA_IN_DNA / MATCHED"""
    missing = sorted(ref_set - dna_set)  # ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô .dna
    extra   = sorted(dna_set - ref_set)  # ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô .dna ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
    matched = sorted(dna_set & ref_set)

    parts = []
    parts += [{"td_id": name, "status": "MISSING_IN_DNA", "berth_id": bid} for bid in missing]
    parts += [{"td_id": name, "status": "EXTRA_IN_DNA",   "berth_id": bid} for bid in extra]
    parts += [{"td_id": name, "status": "MATCHED",        "berth_id": bid} for bid in matched]
    return pd.DataFrame(parts, columns=["td_id", "status", "berth_id"])


def _sanitize_filename(name: str) -> str:
    safe = re.sub(r"[^0-9A-Za-z._-]+", "_", name.strip())
    return safe or "TD"


def _back_to_home():
    st.session_state.page = "home"
    st.rerun()


# ============== HOMEPAGE ==============
def render_home():
    st.title("DVS Tools ‚Ä¢ Homepage")
    st.write("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")

    c1, c2 = st.columns(2)
    with c1:
        if st.button("üè≠ DVS Producer (Berth sorter)", use_container_width=True):
            st.session_state.page = "producer"
            st.rerun()
    with c2:
        if st.button("üß™ DVS Checker", use_container_width=True):
            st.session_state.page = "checker"
            st.rerun()

    st.caption("DVS Producer: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå berth.dna ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ï‡πà‡∏≠ td_id\n"
               "DVS Checker: ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ berth_id ‡∏ï‡∏≤‡∏°‡∏™‡πÄ‡∏õ‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ td (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÅ‡∏≠‡∏õ‡πÄ‡∏î‡∏¥‡∏°)")


# ============== DVS CHECKER (‡∏ï‡∏≤‡∏°‡πÅ‡∏≠‡∏õ‡πÄ‡∏î‡∏¥‡∏°) ==============
def render_checker():
    st.button("‚Üê ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å", on_click=_back_to_home)
    st.title("DVS Checker ‚Ä¢ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö berth_id ‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (Manual paste)")

    dna_files = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .dna (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô)", type=["dna", "txt"], accept_multiple_files=True)
    td_input = st.text_input("‡πÉ‡∏™‡πà‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ td_id (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)", value="Y1 YE FE DR").strip()
    td_list = [t for t in td_input.split() if t]

    st.markdown("### ‡∏ß‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ td (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ space / comma / newline)")
    manual_map = {}
    cols = st.columns(2)
    for i, td in enumerate(td_list):
        with cols[i % 2]:
            manual_map[td] = st.text_area(
                f"{td} ‚Ä¢ ‡∏ß‡∏≤‡∏á `berth_id` ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏õ‡∏Å",
                key=f"ta_{td}",
                height=120,
                placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: A101, A102, A103 ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ/‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î"
            )

    run_btn = st.button("üîé ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô")

    if run_btn:
        if not dna_files:
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .dna ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡πÑ‡∏ü‡∏•‡πå")
            st.stop()

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå .dna
        frames = []
        for f in dna_files:
            try:
                frames.append(parse_dna_file(f))
            except Exception as e:
                st.error(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {f.name} ({e})")
        if not frames:
            st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .dna ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î")
            st.stop()

        df_all = pd.concat(frames, ignore_index=True)

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        summary_rows = []
        per_td_results = {}
        warn_empty = []

        for td in td_list:
            dna_ids = set(df_all.loc[df_all["td_id"] == td, "berth_id"].astype(str))
            ref_ids = parse_manual_list(manual_map.get(td, ""))

            if len(ref_ids) == 0:
                warn_empty.append(td)

            result_df = compare_sets(td, dna_ids, ref_ids)
            per_td_results[td] = result_df

            n_miss  = (result_df["status"] == "MISSING_IN_DNA").sum()
            n_extra = (result_df["status"] == "EXTRA_IN_DNA").sum()
            n_match = (result_df["status"] == "MATCHED").sum()

            summary_rows.append({
                "td_id": td,
                "ref_count": len(ref_ids),
                "dna_count": len(dna_ids),
                "matched": n_match,
                "missing_in_dna": n_miss,
                "extra_in_dna": n_extra
            })

        if warn_empty:
            st.warning("td ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ß‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: " + ", ".join(warn_empty))

        summary = pd.DataFrame(summary_rows)
        st.subheader("üìä SUMMARY")
        st.dataframe(summary, use_container_width=True)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        st.subheader("üîé ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á (‡∏ï‡πà‡∏≠ td_id)")
        for td, dfres in per_td_results.items():
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"**{td} ‚Äî MISSING_IN_DNA** (‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô .dna)")
                st.dataframe(
                    dfres.loc[dfres["status"] == "MISSING_IN_DNA", ["berth_id"]],
                    use_container_width=True, height=240
                )
            with col2:
                st.markdown(f"**{td} ‚Äî EXTRA_IN_DNA** (‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô .dna ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á)")
                st.dataframe(
                    dfres.loc[dfres["status"] == "EXTRA_IN_DNA", ["berth_id"]],
                    use_container_width=True, height=240
                )

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            summary.to_excel(writer, sheet_name="SUMMARY", index=False)
            for td, dfres in per_td_results.items():
                sheet = td[:31] if td else "TD"
                dfres.to_excel(writer, sheet_name=sheet, index=False)
        output.seek(0)

        st.download_button(
            label="‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Excel",
            data=output,
            file_name="td_compare_report.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        st.success("‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        st.caption("Tips: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏™‡πÄ‡∏õ‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ td ‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß copy/paste ‡∏•‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á td ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ")


# ============== DVS PRODUCER (‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ï‡πà‡∏≠ td_id) ==============
def render_producer():
    st.button("‚Üê ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å", on_click=_back_to_home)
    st.title("DVS Producer ‚Ä¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ï‡πà‡∏≠ td_id ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå berth.dna")

    dna_file = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå berth.dna", type=["dna", "txt"], accept_multiple_files=False)
    unique_only = st.checkbox("‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡πà‡∏≤ (recommended)", value=True)
    produce = st.button("üèÅ Produce")

    if produce:
        if dna_file is None:
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .dna ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏î Produce")
            st.stop()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        try:
            df = parse_dna_file(dna_file)
        except Exception as e:
            st.error(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {getattr(dna_file, 'name', 'berth.dna')} ({e})")
            st.stop()

        if df.empty:
            st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .dna")
            st.stop()

        # ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ï‡πà‡∏≠ td_id ‚Üí ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô zip ‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        memzip = BytesIO()
        with zipfile.ZipFile(memzip, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
            for td, grp in df.groupby("td_id"):
                values = grp["berth_id"].astype(str)
                if unique_only:
                    values = pd.Index(values).unique()
                    values = sorted(values)
                content = "\n".join(values) + "\n" if len(values) else ""
                fname = f"{_sanitize_filename(str(td))}.txt"
                zf.writestr(fname, content)

        memzip.seek(0)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        zipname = f"dvs_producer_{ts}.zip"

        st.download_button(
            "‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .zip (‡∏£‡∏ß‡∏° .txt ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° td_id)",
            data=memzip,
            file_name=zipname,
            mime="application/zip",
            use_container_width=True
        )

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πà‡∏≠ td_id
        counts = df.groupby("td_id")["berth_id"].nunique() if unique_only else df.groupby("td_id")["berth_id"].size()
        st.subheader("üì¶ ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠ td_id")
        st.dataframe(counts.rename("count").reset_index(), use_container_width=True)


# ============== ROUTING ==============
if "page" not in st.session_state:
    st.session_state.page = "home"

if st.session_state.page == "home":
    render_home()
elif st.session_state.page == "checker":
    render_checker()
elif st.session_state.page == "producer":
    render_producer()
else:
    render_home()
